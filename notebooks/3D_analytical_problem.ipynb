{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFERNO: Inference-Aware Neural Optimisation\n",
    "\n",
    "This notebook will demonstrate the effectiveness of the\n",
    "*INFERNO: Inference-Aware Neural Optimisation* technique (as described in\n",
    "[arxiv:1806.04743](https://arxiv.org/abs/1806.04743)) on a synthetic 3D inference problem\n",
    "where the effect of nuisance parameters is relevant.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The staple of statistical infence is the is the probability density function\n",
    "$p(\\boldsymbol{x} | \\boldsymbol{\\theta})$,  where $\\boldsymbol{x}$ are\n",
    "the observed data and $\\boldsymbol{\\theta}$ are the parameters. In order to carry obtain inference about a set of i.i.d. observations\n",
    "$D = \\{\\boldsymbol{x}_0, ..., \\boldsymbol{x}_n\\}$,\n",
    "the density\n",
    "$p(\\boldsymbol{x} | \\boldsymbol{\\theta})$ plays a fundamental role linking the\n",
    "observations with the model parameters, both in a classical or a Bayesian setting, because it serves as the basis to construct a likelihood function.\n",
    "\n",
    "However, in many scientific disciplines such as population genetics, epidemiology or\n",
    "experimental particle physics, the only way to realistically model observations and their\n",
    "relation to the model parameters is by means of a complex simulation program.\n",
    "In these cases, the link between the model parameters is only implicitly defined by a\n",
    "generative procedure and statistical inference becomes more challenging.\n",
    "\n",
    "Several approaches such as Approximate Bayesian Computation (ABC) or using\n",
    "simplified synthetic likelihoods instead of the generative likelihood have been developed\n",
    "to tackle the statistical inference problem when only forward simulations are available, in what are referred to as **Likelihood Free Inference** (LFI) techniques (see\n",
    "paper for detailed bibliographic references).\n",
    "\n",
    "If the dimensionality of the data $\\boldsymbol{x}$ is high or the number of\n",
    "observations $n$ is large, \n",
    "statistical inference using likelihood-free techniques can be computationally\n",
    "intractable due to the curse of dimensionality. In these cases, we have to resort to based\n",
    "the inference procedures on a summary statistic of the set of observations\n",
    "$\\boldsymbol{t}(D)$ instead of the original space. The choice of summary statistic\n",
    "is critical and problem-dependent, a naive choice would most likely lead to the loss\n",
    "of information about the parameters and thus a degradation on the resulting statistical\n",
    "inference problem.\n",
    "\n",
    "In this notebook, an approach to learn summary statistics using technique referred as Inference-Aware Neural Optimisation (referred to as INFERNO from now on) is presented and compared with alternatives using a practical benchmark example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "The implementation used in this notebook is heavily based in TensorFlow\n",
    "and TensorFlow Probability, and the code used is freely available in\n",
    "[this GitHub repository](https://github.com/pablodecm/paper-inferno/tree/improve_notebooks_readme).\n",
    "\n",
    "It can either be used locally by installing the environment as described in the repository\n",
    "readme or by using the binder or Google Colab services.\n",
    "If you are seeing this in Google Colab please execute the following cell to install\n",
    "the missing dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 22257,
     "status": "ok",
     "timestamp": 1554914092807,
     "user": {
      "displayName": "Pablo de Castro Manzano",
      "photoUrl": "https://lh5.googleusercontent.com/-MlZ5NSZfqPk/AAAAAAAAAAI/AAAAAAAAAGk/TF2Q-fgyyAw/s64/photo.jpg",
      "userId": "06801920017040372913"
     },
     "user_tz": -120
    },
    "outputId": "979ffb43-1f82-4dc9-81a9-ea9705241134"
   },
   "outputs": [],
   "source": [
    "# only run if executing in Google Colab\n",
    "import os\n",
    "os.environ[\"IS_COLAB\"] = \"True\"\n",
    "!git clone --branch improve_notebooks_readme https://github.com/pablodecm/paper-inferno.git\n",
    "!pip install tensorflow-probability==0.6.0\n",
    "!pip install corner\n",
    "!pip install git+https://github.com/pablodecm/neyman.git@update_deps\n",
    "# update reposity in case it has been updated\n",
    "!cd paper-inferno; git pull\n",
    "# go to folder as if executed locally\n",
    "!cd paper-inferno/notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the dependendencies have been installed, we can proceed to import of the modules that will be used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 24304,
     "status": "ok",
     "timestamp": 1554914094856,
     "user": {
      "displayName": "Pablo de Castro Manzano",
      "photoUrl": "https://lh5.googleusercontent.com/-MlZ5NSZfqPk/AAAAAAAAAAI/AAAAAAAAAGk/TF2Q-fgyyAw/s64/photo.jpg",
      "userId": "06801920017040372913"
     },
     "user_tz": -120
    },
    "outputId": "a7179872-e008-4ec2-c31c-5ac479314337"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import tensorflow as tf\n",
    "# use default matplotlib style\n",
    "plt.style.use('default')\n",
    "\n",
    "from neyman.inferences import batch_hessian\n",
    "import itertools as it\n",
    "from tqdm import tnrange\n",
    "import pandas as pd\n",
    "import corner\n",
    "\n",
    "if \"IS_COLAB\" in os.environ:\n",
    "  repo_path = \"/content/paper-inferno\"\n",
    "else:\n",
    "  repo_path = \"..\"\n",
    "\n",
    "import sys\n",
    "sys.path.append(f\"{repo_path}/code/\")\n",
    "\n",
    "from synthetic_3D_example import SyntheticThreeDimExample\n",
    "from template_model import TemplateModel\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from summary_statistic_computer import SummaryStatisticComputer\n",
    "\n",
    "ds = tfp.distributions\n",
    "ge = tf.contrib.graph_editor\n",
    "k = tf.keras\n",
    "\n",
    "font = {'size'   : 14}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "figure = {'figsize'   : (12,6),\n",
    "          'max_open_warning': False}\n",
    "matplotlib.rc('figure', **figure)\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# this dataframe will be used to keep the final results\n",
    "columns = [\"sigma_s (no nuisances)\", \"sigma_s (w/ nuisances)\"]\n",
    "unc_results = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Description\n",
    "\n",
    "The considered distributions is a mixture of two 3D distributions, referred as\n",
    "$f_s(\\textbf{x})$ and $f_b(\\textbf{x})$. \n",
    "\n",
    "The background distribution $f_s(\\textbf{x})$ consist of a 2D gaussian in the\n",
    "fist two dimensions centred at $(r,0.)$ where r is a nuisance parameter loosely constrained\n",
    "around $r=2.$. The last dimension is a exponential distribution with a rate $\\lambda$ loosely\n",
    "constrained around $\\lambda=3$ which will be the second nuisance parameter.\n",
    "\n",
    "The  signal distribution $f_s(\\textbf{x})$ consist of a 2D gaussian in the\n",
    "fist two dimensions centred at $(0.,0.)$.\n",
    "The last dimension is a exponential distribution with a rate of $2.0$.\n",
    "\n",
    "Analytically the mixture density considered can be expressed as:\n",
    "$$ p(\\textbf{x}) = \\mu \\cdot s(\\textbf{x}) + (1-\\mu) \\cdot b(\\textbf{x}) $$\n",
    "However, a different parametetrization will be considered, in which our parameter of interest\n",
    "will be denoted as $s_{\\textrm{exp}}$ corresponding to the expected number of signal observations, which we expect to be around 100.\n",
    "The expected number of background observations will be the third nuisance parameter, which will\n",
    "be loosely constrained around $b_{\\textrm{exp}} = 1000$. The total number of\n",
    "expected observations will be distributed as $\\textrm{Poisson}(s_{\\textrm{exp}}+b_{\\textrm{exp}}$).\n",
    "The mixture fraction $\\mu$ can be expressed in this parametrization as:\n",
    "$$ \\mu = \\frac{s_{\\textrm{exp}}}{s_{\\textrm{exp}}+b_{\\textrm{exp}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 24304,
     "status": "ok",
     "timestamp": 1554914094858,
     "user": {
      "displayName": "Pablo de Castro Manzano",
      "photoUrl": "https://lh5.googleusercontent.com/-MlZ5NSZfqPk/AAAAAAAAAAI/AAAAAAAAAGk/TF2Q-fgyyAw/s64/photo.jpg",
      "userId": "06801920017040372913"
     },
     "user_tz": -120
    },
    "outputId": "4b84dea1-c699-4b19-c6c1-44ade106c3c5"
   },
   "outputs": [],
   "source": [
    "\n",
    "def show_function_code(function):\n",
    "  \n",
    "    import inspect\n",
    "    from pygments import highlight\n",
    "    from pygments.lexers import PythonLexer\n",
    "    from pygments.formatters import HtmlFormatter\n",
    "    from IPython.core.display import HTML\n",
    "    \n",
    "    source_code = \"\".join(inspect.getsourcelines(function)[0])\n",
    "\n",
    "    return HTML(highlight(source_code, PythonLexer(), HtmlFormatter(full=True)))\n",
    "  \n",
    "\n",
    "show_function_code(SyntheticThreeDimExample.__init__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 24304,
     "status": "ok",
     "timestamp": 1554914094859,
     "user": {
      "displayName": "Pablo de Castro Manzano",
      "photoUrl": "https://lh5.googleusercontent.com/-MlZ5NSZfqPk/AAAAAAAAAAI/AAAAAAAAAGk/TF2Q-fgyyAw/s64/photo.jpg",
      "userId": "06801920017040372913"
     },
     "user_tz": -120
    },
    "outputId": "902e77d8-a83b-4f6a-8251-d86444d7c0b6"
   },
   "outputs": [],
   "source": [
    "show_function_code(SyntheticThreeDimExample.transform_bkg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = SyntheticThreeDimExample()\n",
    "\n",
    "x_values = tf.placeholder(dtype=tf.float32, shape=(None, 3), name=\"x_values\")\n",
    "\n",
    "phs = { problem.r_dist : 2.,\n",
    "        problem.b_rate : 3.,\n",
    "        problem.b_exp : 1000,\n",
    "        problem.s_exp : 50}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 27186,
     "status": "ok",
     "timestamp": 1554914097744,
     "user": {
      "displayName": "Pablo de Castro Manzano",
      "photoUrl": "https://lh5.googleusercontent.com/-MlZ5NSZfqPk/AAAAAAAAAAI/AAAAAAAAAGk/TF2Q-fgyyAw/s64/photo.jpg",
      "userId": "06801920017040372913"
     },
     "user_tz": -120
    },
    "outputId": "052d4201-7f30-4b66-b16a-b415e7388069"
   },
   "outputs": [],
   "source": [
    "\n",
    "n_s = 50000\n",
    "\n",
    "s_sam = problem.s_dist.sample(n_s, seed=27)\n",
    "b_sam = problem.b_dist.sample(n_s, seed=27)\n",
    "with tf.Session() as sess:\n",
    "  b_sample = sess.run(b_sam, phs)\n",
    "  s_sample = sess.run(s_sam, phs)\n",
    "\n",
    "labels = [r\"$x_0$\", \"$x_1$\", \"$x_2$\"]\n",
    "ran = [(-10,10),\n",
    "       (-10,10),\n",
    "       (0,4)]\n",
    "bins = [np.linspace(-10,10,40, endpoint=True),\n",
    "        np.linspace(-10,10,40, endpoint=True),\n",
    "        np.linspace(0,4,20, endpoint=True)]\n",
    "\n",
    "# 2D levels to use\n",
    "levels = 1.0 - np.exp(-0.5 * np.array([1.,2.,3.]) ** 2)\n",
    "\n",
    "fig = corner.corner(b_sample, bins=bins,range=ran, color=\"blue\",weights=np.ones(n_s),\n",
    "                    smooth=0.95, labels=labels,levels=levels,plot_datapoints=False)\n",
    "fig = corner.corner(s_sample, bins=bins,range=ran, color=\"red\",weights=np.ones(n_s),\n",
    "                    smooth=0.95, labels=labels,levels=levels,plot_datapoints=False,\n",
    "                    fig=fig)\n",
    "\n",
    "# set 1D hist y-scales\n",
    "scales = [12000, 12000, 25000]\n",
    "n_dim = 3\n",
    "axes = np.array(fig.axes).reshape((n_dim, n_dim))\n",
    "for i in range(n_dim):\n",
    "  ax = axes[i, i]\n",
    "  ax.set_ylim([0,scales[i]])\n",
    "\n",
    "#fig.savefig(\"../paper/gfx/figure2a.pdf\",bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 28827,
     "status": "ok",
     "timestamp": 1554914099386,
     "user": {
      "displayName": "Pablo de Castro Manzano",
      "photoUrl": "https://lh5.googleusercontent.com/-MlZ5NSZfqPk/AAAAAAAAAAI/AAAAAAAAAGk/TF2Q-fgyyAw/s64/photo.jpg",
      "userId": "06801920017040372913"
     },
     "user_tz": -120
    },
    "outputId": "3079f671-6e16-45af-fa2f-cd1a9b7f01e2"
   },
   "outputs": [],
   "source": [
    "\n",
    "n_s = 50000\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  weights=np.ones(s_sample.shape[0])\n",
    "  m_sample = sess.run(problem.m_dist.sample(n_s, seed=17), phs)\n",
    "\n",
    "labels = [r\"$x_0$\", \"$x_1$\", \"$x_2$\"]\n",
    "ran = [(-10,10),\n",
    "       (-10,10),\n",
    "       (0,4)]\n",
    "bins = [np.linspace(-10,10,40, endpoint=True),\n",
    "        np.linspace(-10,10,40, endpoint=True),\n",
    "        np.linspace(0,4,20, endpoint=True)]\n",
    "\n",
    "# 2D levels to use\n",
    "levels = 1.0 - np.exp(-0.5 * np.array([1.,2.,3.]) ** 2)\n",
    "\n",
    "fig = corner.corner(m_sample, bins=bins, range=ran,\n",
    "                    levels=levels,color=\"black\",weights=np.ones(n_s),\n",
    "                    smooth=0.9999, labels=labels,plot_datapoints=False)\n",
    "\n",
    "# set 1D hist y-scales\n",
    "scales = [12000, 12000, 25000]\n",
    "n_dim = 3\n",
    "axes = np.array(fig.axes).reshape((n_dim, n_dim))\n",
    "for i in range(n_dim):\n",
    "  ax = axes[i, i]\n",
    "  ax.set_ylim([0,scales[i]])\n",
    "\n",
    "#fig.savefig(\"../paper/gfx/figure2b.pdf\",bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "  train_arrays = sess.run(problem.train_data())\n",
    "  valid_arrays = sess.run(problem.valid_data())   \n",
    "  test_arrays =  sess.run(problem.test_data())   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 679642,
     "status": "ok",
     "timestamp": 1554914750202,
     "user": {
      "displayName": "Pablo de Castro Manzano",
      "photoUrl": "https://lh5.googleusercontent.com/-MlZ5NSZfqPk/AAAAAAAAAAI/AAAAAAAAAGk/TF2Q-fgyyAw/s64/photo.jpg",
      "userId": "06801920017040372913"
     },
     "user_tz": -120
    },
    "outputId": "d87e9eaa-ad5c-4c5e-95be-3edf552b89f6"
   },
   "outputs": [],
   "source": [
    "from synthetic_3D_cross_entropy import SyntheticThreeDimCrossEntropy\n",
    "\n",
    "clf_name = \"NN Classifier\"\n",
    "seed = 17\n",
    "\n",
    "n_epochs = 100\n",
    "lr = 1e-3\n",
    "batch_size = 32\n",
    "xe_path = f\"xe_lr_{lr}_n_e_{n_epochs}_seed_{seed}\"\n",
    "\n",
    "xe_model = SyntheticThreeDimCrossEntropy(model_path=xe_path,\n",
    "                                         seed=seed)\n",
    "\n",
    "xe_model.fit(n_epochs=n_epochs, lr=lr,batch_size=batch_size, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc = SummaryStatisticComputer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_shapes = ssc.classifier_shapes(xe_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from template_model import TemplateModel\n",
    "\n",
    "def compute_fisher(shapes):\n",
    "  with tf.Session() as sess:\n",
    "    tm = TemplateModel()\n",
    "    tm.templates_from_dict(shapes)\n",
    "    fisher = tm.asimov_hess()\n",
    "  return fisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 685178,
     "status": "ok",
     "timestamp": 1554914755741,
     "user": {
      "displayName": "Pablo de Castro Manzano",
      "photoUrl": "https://lh5.googleusercontent.com/-MlZ5NSZfqPk/AAAAAAAAAAI/AAAAAAAAAGk/TF2Q-fgyyAw/s64/photo.jpg",
      "userId": "06801920017040372913"
     },
     "user_tz": -120
    },
    "outputId": "56002984-0ad6-4d48-bc82-4f7ed34f91f4"
   },
   "outputs": [],
   "source": [
    "clf_fish = compute_fisher(clf_shapes)\n",
    "sigma_s_no_nuis = clf_fish.marginals([\"s_exp\"])[\"s_exp\"]\n",
    "sigma_s_w_nuis = clf_fish.marginals([\"s_exp\",\"r_dist\",\"b_rate\"])[\"s_exp\"]\n",
    "unc_results.loc[clf_name] = [sigma_s_no_nuis, sigma_s_w_nuis]\n",
    "unc_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "s_exp_scan = np.linspace(20.,80.,61, endpoint=True)\n",
    "pars = [\"r_dist\",\"b_rate\"]\n",
    "n_steps = 5\n",
    "d = 0\n",
    "step_size = 0.1\n",
    "\n",
    "def profile_likelihood(shapes, pars = [\"r_dist\",\"b_rate\"]):\n",
    "  \n",
    "  with tf.Session() as sess:\n",
    "    tm = TemplateModel(multiple_pars=True)\n",
    "    tm.templates_from_dict(shapes)\n",
    "    par_phs = {tm.r_dist : 2.0*np.ones_like(s_exp_scan),\n",
    "             tm.b_rate : 3.0*np.ones_like(s_exp_scan),\n",
    "             tm.s_exp :  s_exp_scan,\n",
    "             tm.b_exp : 1000.0*np.ones_like(s_exp_scan)} \n",
    "   \n",
    "  \n",
    "    asimov_data = tm.asimov_data(sess=sess)\n",
    "  \n",
    "    obs_phs = {tm.obs : asimov_data}\n",
    "    mod_phs = par_phs.copy()\n",
    "    # get likelihood before changing pars\n",
    "    nll, sub_hess, sub_grad = tm.hessian_and_gradient(pars=pars,\n",
    "                                                      par_phs=mod_phs, obs_phs=obs_phs)\n",
    "      \n",
    "    # profile likelihood with Newton method\n",
    "    for i in range(n_steps):\n",
    "      newton_step =  np.matmul(np.linalg.inv(sub_hess+d*np.eye(len(pars))),sub_grad[:,:,np.newaxis])\n",
    "      mod_phs[tm.r_dist] = mod_phs[tm.r_dist] - step_size*newton_step[:,0,0]\n",
    "      mod_phs[tm.b_rate] = mod_phs[tm.b_rate] - step_size*newton_step[:,1,0]\n",
    "      p_nll, sub_hess, sub_grad = tm.hessian_and_gradient(pars=[\"r_dist\",\"b_rate\"],\n",
    "                                                          par_phs=mod_phs, obs_phs=obs_phs)\n",
    "      \n",
    "    return nll, p_nll\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 690095,
     "status": "ok",
     "timestamp": 1554914760660,
     "user": {
      "displayName": "Pablo de Castro Manzano",
      "photoUrl": "https://lh5.googleusercontent.com/-MlZ5NSZfqPk/AAAAAAAAAAI/AAAAAAAAAGk/TF2Q-fgyyAw/s64/photo.jpg",
      "userId": "06801920017040372913"
     },
     "user_tz": -120
    },
    "outputId": "d83ad040-b94c-4816-92c5-190ca9ce6917"
   },
   "outputs": [],
   "source": [
    "nll, p_nll = profile_likelihood(clf_shapes)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(s_exp_scan, nll, label=\"no nuisances\" )\n",
    "ax.plot(s_exp_scan, p_nll, label=\"w/ nuisances\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_variation_plot(shape_dict, bins=None):\n",
    "  fig, ax = plt.subplots(2,1, figsize=(8,8),sharex=True)\n",
    "  fig.subplots_adjust(hspace = 0.05)\n",
    "  \n",
    "  \n",
    "  if bins is None: \n",
    "    bins = np.linspace(0,1,11,endpoint=True)\n",
    "  centers =   (bins[1:]+bins[:-1])/2.\n",
    "  width = (bins[1:]-bins[:-1])\n",
    "  \n",
    "  plot_options = {(\"sig\",) : (r\"signal\",\"red\"),\n",
    "                 (\"bkg\",2.,3.) : (r\"background\",\"blue\")}\n",
    "                 \n",
    "  for k, option in plot_options.items():\n",
    "    clf_shapes_norm = shape_dict[k]/shape_dict[k].sum()\n",
    "    ax[0].bar(x=centers, height=clf_shapes_norm,\n",
    "              width=width,color=option[1], alpha=0.35,\n",
    "              label=option[0])\n",
    "  \n",
    "  exp_bkg =  (shape_dict[(\"bkg\",2.,3.)]/shape_dict[(\"bkg\",2.,3.)].sum())*1000.\n",
    "  exp_sig = (shape_dict[(\"sig\",)]/shape_dict[(\"sig\",)].sum())*50.\n",
    "  sig_art = ax[1].bar(x=centers, height=exp_sig/exp_bkg,width=width,\n",
    "                      color=\"red\", label=\"signal\", alpha=0.35)\n",
    "  \n",
    "  plot_options = {(\"bkg\",2.0,2.5) : (r\"v\",\"green\"),\n",
    "                  (\"bkg\",2.0,3.5) : (r\"^\",\"green\"),\n",
    "                  (\"bkg\",1.8,3.0) : (r\"v\",\"purple\"),\n",
    "                  (\"bkg\",2.2,3.0) : (r\"^\",\"purple\")}\n",
    "  \n",
    "  var_lines = []\n",
    "  var_labels = []\n",
    "  for k, option in plot_options.items():\n",
    "    exp_shift_bkg = (shape_dict[k]/shape_dict[k].sum())*1000.\n",
    "    label = f\"bkg $r={k[1]}$ $\\\\lambda={k[2]}$\"\n",
    "    var_lines.append(ax[1].errorbar(x=centers, y=(exp_shift_bkg-exp_bkg)/exp_bkg,\n",
    "                     xerr=width/2.,fmt=\".\", color=option[1],\n",
    "                     alpha=0.55, marker=option[0]))\n",
    "    var_labels.append(label)\n",
    "  \n",
    "  ax[0].legend(loc=\"upper center\", frameon=False)\n",
    "  ax[0].set_ylim(top=0.7)\n",
    "  ax[0].set_ylabel(\"counts (normalised)\")\n",
    "  \n",
    "  leg_0 = ax[1].legend(var_lines,var_labels,loc=\"lower center\", ncol=2,frameon=False)\n",
    "  leg_1 = ax[1].legend([sig_art],[\"signal\"],loc=\"upper center\", frameon=False)\n",
    "\n",
    "  ax[1].set_ylim([-0.75,0.75])\n",
    "  ax[1].set_xlim([bins.min(),bins.max()])\n",
    "  \n",
    "  ax[1].add_artist(leg_0)\n",
    "  ax[1].set_ylabel(\"relative variation\")\n",
    "  ax[1].yaxis.set_major_locator(matplotlib.ticker.MultipleLocator(0.2))\n",
    "  \n",
    "  fig.align_ylabels()\n",
    "\n",
    "  \n",
    "  return ax, fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 690989,
     "status": "ok",
     "timestamp": 1554914761555,
     "user": {
      "displayName": "Pablo de Castro Manzano",
      "photoUrl": "https://lh5.googleusercontent.com/-MlZ5NSZfqPk/AAAAAAAAAAI/AAAAAAAAAGk/TF2Q-fgyyAw/s64/photo.jpg",
      "userId": "06801920017040372913"
     },
     "user_tz": -120
    },
    "outputId": "9c443f15-37d5-41b0-d9d2-e5e03dd1877f"
   },
   "outputs": [],
   "source": [
    "axs, fig = shape_variation_plot(clf_shapes)\n",
    "axs[1].set_xlabel(\"neural network classifier output\")\n",
    "#fig.savefig(\"../paper/gfx/figure3a.pdf\",bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytical likelihood ratio\n",
    "\n",
    "Probabilistic classifiers trained to differenciate signal and background observation can be used to approximate the likelihood ratio. In this\n",
    "particular case, we can easily approximate the likelihood ratio analytically.\n",
    "\n",
    "$$t_\\frac{s}{s+b}(\\textbf{x}) = \\frac{f_s(\\textbf{x})}{f_s(\\textbf{x}) + f_b(\\textbf{x})}$$\n",
    "\n",
    "where $f_s$ is fully specified but $f_b$ depends on $r$ and $\\lambda$, which for\n",
    "a probabilistic classifier are typically taken as fixed at certain values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 695892,
     "status": "ok",
     "timestamp": 1554914766460,
     "user": {
      "displayName": "Pablo de Castro Manzano",
      "photoUrl": "https://lh5.googleusercontent.com/-MlZ5NSZfqPk/AAAAAAAAAAI/AAAAAAAAAGk/TF2Q-fgyyAw/s64/photo.jpg",
      "userId": "06801920017040372913"
     },
     "user_tz": -120
    },
    "outputId": "3173af56-9b89-44ac-9bcf-30bec4abef0c"
   },
   "outputs": [],
   "source": [
    "opt_name = \"Optimal Classifier\"\n",
    "\n",
    "opt_shapes = ssc.optimal_shapes()\n",
    "opt_fish = compute_fisher(opt_shapes)\n",
    "\n",
    "sigma_s_no_nuis = opt_fish.marginals([\"s_exp\"])[\"s_exp\"]\n",
    "sigma_s_w_nuis = opt_fish.marginals([\"s_exp\",\"r_dist\",\"b_rate\"])[\"s_exp\"]\n",
    "unc_results.loc[opt_name] = [sigma_s_no_nuis, sigma_s_w_nuis]\n",
    "unc_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 696671,
     "status": "ok",
     "timestamp": 1554914767239,
     "user": {
      "displayName": "Pablo de Castro Manzano",
      "photoUrl": "https://lh5.googleusercontent.com/-MlZ5NSZfqPk/AAAAAAAAAAI/AAAAAAAAAGk/TF2Q-fgyyAw/s64/photo.jpg",
      "userId": "06801920017040372913"
     },
     "user_tz": -120
    },
    "outputId": "eda1a007-a974-45f6-d6cb-34ce0e5348ce"
   },
   "outputs": [],
   "source": [
    "axs, fig = shape_variation_plot(opt_shapes)\n",
    "\n",
    "axs[1].set_xlabel(\"optimal classifier output\")\n",
    "#fig.savefig(\"../paper/gfx/figure3b.pdf\",bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INFERNO training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 349758,
     "status": "ok",
     "timestamp": 1554915804326,
     "user": {
      "displayName": "Pablo de Castro Manzano",
      "photoUrl": "https://lh5.googleusercontent.com/-MlZ5NSZfqPk/AAAAAAAAAAI/AAAAAAAAAGk/TF2Q-fgyyAw/s64/photo.jpg",
      "userId": "06801920017040372913"
     },
     "user_tz": -120
    },
    "outputId": "d0289e05-c1cb-4873-a7be-ba952069b129"
   },
   "outputs": [],
   "source": [
    "from synthetic_3D_inferno import SyntheticThreeDimInferno\n",
    "\n",
    "inf_name = \"INFERNO\"\n",
    "\n",
    "n_epochs = 200\n",
    "lr = 1e-6\n",
    "batch_size = 1000\n",
    "t_train = 0.1\n",
    "t_eval = 0.05\n",
    "\n",
    "n_inits = 100\n",
    "seed = 7\n",
    "\n",
    "\n",
    "pars = [\"s_exp\", \"r_dist\", \"b_rate\"]\n",
    "\n",
    "\n",
    "inf_path = f\"inf_ne_{n_epochs}_lr_{lr}_bs_{batch_size}_t_{t_train}\"\n",
    "\n",
    "inferno = SyntheticThreeDimInferno(model_path=inf_path, poi=\"s_exp\",\n",
    "                                    pars=pars, seed=seed)\n",
    "inferno.fit(n_epochs=n_epochs, lr=lr, batch_size=batch_size,\n",
    "            temperature=t_train, seed=seed)\n",
    "\n",
    "inf_fisher, inf_aux_fisher = inferno.eval_hessian(temperature=t_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 767,
     "status": "ok",
     "timestamp": 1554915805090,
     "user": {
      "displayName": "Pablo de Castro Manzano",
      "photoUrl": "https://lh5.googleusercontent.com/-MlZ5NSZfqPk/AAAAAAAAAAI/AAAAAAAAAGk/TF2Q-fgyyAw/s64/photo.jpg",
      "userId": "06801920017040372913"
     },
     "user_tz": -120
    },
    "outputId": "8350ec86-a3d4-4887-805f-a2299793876a"
   },
   "outputs": [],
   "source": [
    "loss_valid = np.array(inferno.history[\"loss_valid\"])\n",
    "plt.plot(loss_valid[:,0], loss_valid[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_shapes = ssc.inferno_shapes(inf_path)\n",
    "inf_fish = compute_fisher(inf_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1554915813122,
     "user": {
      "displayName": "Pablo de Castro Manzano",
      "photoUrl": "https://lh5.googleusercontent.com/-MlZ5NSZfqPk/AAAAAAAAAAI/AAAAAAAAAGk/TF2Q-fgyyAw/s64/photo.jpg",
      "userId": "06801920017040372913"
     },
     "user_tz": -120
    },
    "outputId": "fdaf8b80-727a-42bb-dced-356b41ff0d38"
   },
   "outputs": [],
   "source": [
    "inf_fish.marginals([\"s_exp\"])[\"s_exp\"]\n",
    "sigma_s_no_nuis = inf_fish.marginals([\"s_exp\"])[\"s_exp\"]\n",
    "sigma_s_w_nuis = inf_fish.marginals([\"s_exp\",\"r_dist\",\"b_rate\"])[\"s_exp\"]\n",
    "unc_results.loc[inf_name] = [sigma_s_no_nuis, sigma_s_w_nuis]\n",
    "\n",
    "unc_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7440,
     "status": "ok",
     "timestamp": 1554922976953,
     "user": {
      "displayName": "Pablo de Castro Manzano",
      "photoUrl": "https://lh5.googleusercontent.com/-MlZ5NSZfqPk/AAAAAAAAAAI/AAAAAAAAAGk/TF2Q-fgyyAw/s64/photo.jpg",
      "userId": "06801920017040372913"
     },
     "user_tz": -120
    },
    "outputId": "99b8fe55-d8dc-4857-d083-4da84c67e5fe"
   },
   "outputs": [],
   "source": [
    "nll, p_nll = profile_likelihood(inf_shapes)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(s_exp_scan, nll, label=\"no nuisances\" )\n",
    "ax.plot(s_exp_scan, p_nll, label=\"w/ nuisances\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1592,
     "status": "ok",
     "timestamp": 1554922984853,
     "user": {
      "displayName": "Pablo de Castro Manzano",
      "photoUrl": "https://lh5.googleusercontent.com/-MlZ5NSZfqPk/AAAAAAAAAAI/AAAAAAAAAGk/TF2Q-fgyyAw/s64/photo.jpg",
      "userId": "06801920017040372913"
     },
     "user_tz": -120
    },
    "outputId": "1d9c402e-2872-4df9-fd63-ad96525bcd98"
   },
   "outputs": [],
   "source": [
    "axs, fig = shape_variation_plot(inf_shapes)\n",
    "\n",
    "axs[1].set_xlabel(\"inferno output\")\n",
    "#axs[0].set_ylim(top=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytical Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extended_model import ExtendedModel\n",
    "\n",
    "lik_name = \"Analytical Inference\"\n",
    "problem = SyntheticThreeDimExample()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = tf.placeholder(dtype=tf.float32, shape=(None, 3), name=\"x_values\")\n",
    "\n",
    "em = ExtendedModel(problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_t = problem.transform_bkg(x_values)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  valid_arrays = sess.run(problem.valid_data())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "  bkg_t_arr = sess.run(bkg_t, {x_values : valid_arrays[\"bkg\"]})\n",
    "  obs_phs = {em.s_n_exp : 50.,\n",
    "               em.b_n_exp : 1000.,\n",
    "               em.s_data : valid_arrays[\"sig\"],\n",
    "               em.b_data : bkg_t_arr }\n",
    "  e_hess, e_hess_aux = em.hess(par_phs={},obs_phs=obs_phs)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11413,
     "status": "ok",
     "timestamp": 1554923003887,
     "user": {
      "displayName": "Pablo de Castro Manzano",
      "photoUrl": "https://lh5.googleusercontent.com/-MlZ5NSZfqPk/AAAAAAAAAAI/AAAAAAAAAGk/TF2Q-fgyyAw/s64/photo.jpg",
      "userId": "06801920017040372913"
     },
     "user_tz": -120
    },
    "outputId": "fda0e8b0-5cfb-400f-9736-c899c58ea19c"
   },
   "outputs": [],
   "source": [
    "e_hess.marginals([\"s_exp\"])[\"s_exp\"]\n",
    "\n",
    "sigma_s_no_nuis = e_hess.marginals([\"s_exp\"])[\"s_exp\"]\n",
    "sigma_s_w_nuis = e_hess.marginals([\"s_exp\",\"r_dist\",\"b_rate\"])[\"s_exp\"]\n",
    "unc_results.loc[lik_name] = [sigma_s_no_nuis, sigma_s_w_nuis]\n",
    "\n",
    "unc_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BYOS: Bring your Own Summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the example of a hand-made summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12801,
     "status": "ok",
     "timestamp": 1554923007074,
     "user": {
      "displayName": "Pablo de Castro Manzano",
      "photoUrl": "https://lh5.googleusercontent.com/-MlZ5NSZfqPk/AAAAAAAAAAI/AAAAAAAAAGk/TF2Q-fgyyAw/s64/photo.jpg",
      "userId": "06801920017040372913"
     },
     "user_tz": -120
    },
    "outputId": "3e34be56-383b-40e0-ec4c-3db0fb6a3cf1"
   },
   "outputs": [],
   "source": [
    "naive_name = \"Naive Summary\"\n",
    "\n",
    "def naive_summary(data):\n",
    "  return np.log(np.abs(data[:,1]))\n",
    "\n",
    "\n",
    "# you can also choose the binning \n",
    "bins = np.linspace(0,2.5,11,endpoint=True)\n",
    "naive_shapes = ssc.generic_shapes(naive_summary,bins)\n",
    "\n",
    "axs, fig = shape_variation_plot(naive_shapes, bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 18251,
     "status": "ok",
     "timestamp": 1554923012998,
     "user": {
      "displayName": "Pablo de Castro Manzano",
      "photoUrl": "https://lh5.googleusercontent.com/-MlZ5NSZfqPk/AAAAAAAAAAI/AAAAAAAAAGk/TF2Q-fgyyAw/s64/photo.jpg",
      "userId": "06801920017040372913"
     },
     "user_tz": -120
    },
    "outputId": "d41e4255-9cbe-4f4d-e9bb-efe80be89d7d"
   },
   "outputs": [],
   "source": [
    "naive_fish = compute_fisher(naive_shapes)\n",
    "# we do not need to consider the r_dist or b_rate parameters because only\n",
    "# affects the first and third dimension respectively\n",
    "# In fact it will make the inverse Hessian singular because this naive\n",
    "# summary statistic does not provide information about those nuisance parameters\n",
    "\n",
    "naive_fish.marginals([\"s_exp\"])[\"s_exp\"]\n",
    "sigma_s_w_nuis = naive_fish.marginals([\"s_exp\"])[\"s_exp\"]\n",
    "unc_results.loc[naive_name] = [sigma_s_no_nuis, sigma_s_w_nuis]\n",
    "\n",
    "unc_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively we could use machine learning model trained with other library or method, here I provide a working PyTorch based binary classifier as a skeleton example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 49039,
     "status": "ok",
     "timestamp": 1554923045972,
     "user": {
      "displayName": "Pablo de Castro Manzano",
      "photoUrl": "https://lh5.googleusercontent.com/-MlZ5NSZfqPk/AAAAAAAAAAI/AAAAAAAAAGk/TF2Q-fgyyAw/s64/photo.jpg",
      "userId": "06801920017040372913"
     },
     "user_tz": -120
    },
    "outputId": "6d61b32d-a602-49a0-bdd8-812f1672d505"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as utils\n",
    "\n",
    "class Model(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Model, self).__init__()\n",
    "    self.fc1 = nn.Linear(3,50)\n",
    "    self.fc2 = nn.Linear(50,100)\n",
    "    self.fc3 = nn.Linear(100,100)\n",
    "    self.out = nn.Linear(100,1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = F.relu(self.fc2(x))\n",
    "    x = F.relu(self.fc3(x))\n",
    "    return torch.sigmoid(self.out(x))\n",
    "\n",
    "\n",
    "model = Model()\n",
    "opt = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.BCELoss()\n",
    " \n",
    "X = np.vstack([train_arrays[\"sig\"],train_arrays[\"bkg\"]])\n",
    "y = np.hstack([np.ones(train_arrays[\"sig\"].shape[0], dtype=np.float32),\n",
    "               np.zeros(train_arrays[\"bkg\"].shape[0], dtype=np.float32)])[:,np.newaxis]\n",
    "\n",
    "permutation = np.random.permutation(X.shape[0])\n",
    "X = torch.as_tensor(X[permutation])\n",
    "y = torch.as_tensor(y[permutation])\n",
    "\n",
    "trainset = utils.TensorDataset(X,y)\n",
    "trainloader = utils.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "loss_history = []\n",
    "loss_batch_number = []\n",
    "batch_number = 0\n",
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, target = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        batch_number += 1\n",
    "        if i % 100 == 99:    # print every 100 mini-batches\n",
    "            loss_history.append(running_loss/100.)\n",
    "            loss_batch_number.append(batch_number)\n",
    "            running_loss = 0.0\n",
    "\n",
    "plt.plot(loss_batch_number,loss_history);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 52311,
     "status": "ok",
     "timestamp": 1554923050630,
     "user": {
      "displayName": "Pablo de Castro Manzano",
      "photoUrl": "https://lh5.googleusercontent.com/-MlZ5NSZfqPk/AAAAAAAAAAI/AAAAAAAAAGk/TF2Q-fgyyAw/s64/photo.jpg",
      "userId": "06801920017040372913"
     },
     "user_tz": -120
    },
    "outputId": "03eb9a3d-4f60-4301-c021-8314dd23d126"
   },
   "outputs": [],
   "source": [
    "byos_name = \"BYOS Summary\"\n",
    "\n",
    "def byos_summary(data):\n",
    "  # you can use anything that transforms each observation to a one dimensional\n",
    "  # summary (e.g. a trained model neural network model output using your\n",
    "  # preferred library)\n",
    "  # both data and transformed_data are numpy arrays\n",
    "  \n",
    "  # could also be loaded from a file\n",
    "  transformed_data = model(torch.as_tensor(data)).data\n",
    "  return transformed_data\n",
    "\n",
    "\n",
    "# you can also choose the binning so it is adecuate for your summary\n",
    "bins = np.linspace(0,1,11,endpoint=True)\n",
    "\n",
    "# compute shapes and variation\n",
    "byos_shapes = ssc.generic_shapes(byos_summary,bins)\n",
    "axs, fig = shape_variation_plot(byos_shapes, bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 57659,
     "status": "ok",
     "timestamp": 1554923056626,
     "user": {
      "displayName": "Pablo de Castro Manzano",
      "photoUrl": "https://lh5.googleusercontent.com/-MlZ5NSZfqPk/AAAAAAAAAAI/AAAAAAAAAGk/TF2Q-fgyyAw/s64/photo.jpg",
      "userId": "06801920017040372913"
     },
     "user_tz": -120
    },
    "outputId": "4be554ae-b969-4577-b0ba-5a4ed9605e88"
   },
   "outputs": [],
   "source": [
    "byos_fish = compute_fisher(byos_shapes)\n",
    "\n",
    "sigma_s_no_nuis = byos_fish.marginals([\"s_exp\"])[\"s_exp\"]\n",
    "sigma_s_w_nuis = byos_fish.marginals([\"s_exp\",\"r_dist\",\"b_rate\"])[\"s_exp\"]\n",
    "unc_results.loc[byos_name] = [sigma_s_no_nuis, sigma_s_w_nuis]\n",
    "\n",
    "unc_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 64868,
     "status": "ok",
     "timestamp": 1554923064778,
     "user": {
      "displayName": "Pablo de Castro Manzano",
      "photoUrl": "https://lh5.googleusercontent.com/-MlZ5NSZfqPk/AAAAAAAAAAI/AAAAAAAAAGk/TF2Q-fgyyAw/s64/photo.jpg",
      "userId": "06801920017040372913"
     },
     "user_tz": -120
    },
    "outputId": "84454806-c7a0-499c-8040-568d1a715711"
   },
   "outputs": [],
   "source": [
    "nll, p_nll = profile_likelihood(byos_shapes)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(s_exp_scan, nll, label=\"no nuisances\" )\n",
    "ax.plot(s_exp_scan, p_nll, label=\"w/ nuisances\")\n",
    "ax.legend()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

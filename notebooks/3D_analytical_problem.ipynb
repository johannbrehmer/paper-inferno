{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only run if executing in Google Colab\n",
    "import os\n",
    "os.environ[\"IS_COLAB\"] = \"True\"\n",
    "!git clone https://github.com/pablodecm/paper-inferno.git\n",
    "!pip install tensorflow-probability==0.6.0\n",
    "!pip install corner\n",
    "!pip install git+https://github.com/pablodecm/neyman.git@update_deps\n",
    "# go to folder as if executed locally\n",
    "!cd paper-inferno/notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import tensorflow as tf\n",
    "# use classic as base\n",
    "plt.style.use('default')\n",
    "\n",
    "from neyman.inferences import batch_hessian\n",
    "import itertools as it\n",
    "from tqdm import tnrange\n",
    "import corner\n",
    "\n",
    "if \"IS_COLAB\" in os.environ:\n",
    "  repo_path = \"/content/paper-inferno\"\n",
    "else:\n",
    "  repo_path = \"..\"\n",
    "\n",
    "import sys\n",
    "sys.path.append(f\"{repo_path}/code/\")\n",
    "\n",
    "from synthetic_3D_example import SyntheticThreeDimExample\n",
    "from template_model import TemplateModel\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from summary_statistic_computer import SummaryStatisticComputer\n",
    "\n",
    "ds = tfp.distributions\n",
    "ge = tf.contrib.graph_editor\n",
    "k = tf.keras\n",
    "\n",
    "font = {'size'   : 14}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "figure = {'figsize'   : (12,8),\n",
    "          'max_open_warning': False}\n",
    "matplotlib.rc('figure', **figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem description\n",
    "\n",
    "The considered distributions is a mixture of two 3D distributions, referred as $s(\\textbf{x})$\n",
    "and $b(\\textbf{x})$. \n",
    "\n",
    "The background distribution $s(\\textbf{x})$ consist of a 2D gaussian in the\n",
    "fist two dimensions centred at $(r,0.)$ where r is a nuissance parameter loosely constrained\n",
    "around $r=2.$. The last dimension is a exponential distribution with a rate $\\lambda$ loosely\n",
    "constrained around $\\lambda=3$ which will be the second nuissance parameter.\n",
    "\n",
    "The  signal distribution $s(\\textbf{x})$ consist of a 2D gaussian in the\n",
    "fist two dimensions centred at $(0.,0.)$.\n",
    "The last dimension is a exponential distribution with a rate of $2.0$.\n",
    "\n",
    "Analytically the mixture density considered can be expressed as:\n",
    "$$ m(\\textbf{x}) = \\mu \\cdot s(\\textbf{x}) + (1-\\mu) \\cdot b(\\textbf{x}) $$\n",
    "However, a different parametetrization will be considered, in which our parameter of interest\n",
    "will be denoted as $s_{\\textrm{exp}}$ corresponding to the expected number of signal observations, which we expect to be around 100.\n",
    "The expected number of background observations will be the third nuissance parameter, which will\n",
    "be loosely constrained around $b_{\\textrm{exp}} = 1000$. The total number of\n",
    "expected observations will be distributed as $\\textrm{Poisson}(s_{\\textrm{exp}}+b_{\\textrm{exp}}$). The mixture fraction $\\mu$ can be expressed in this parametrization as:\n",
    "$$ \\mu = \\frac{s_{\\textrm{exp}}}{s_{\\textrm{exp}}+b_{\\textrm{exp}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SyntheticThreeDimExample.__init__??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SyntheticThreeDimExample.transform_bkg??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "50./1050."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = SyntheticThreeDimExample()\n",
    "\n",
    "x_values = tf.placeholder(dtype=tf.float32, shape=(None, 3), name=\"x_values\")\n",
    "\n",
    "phs = { problem.r_dist : 2.,\n",
    "        problem.b_rate : 3.,\n",
    "        problem.b_exp : 1000,\n",
    "        problem.s_exp : 50}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_s = 50000\n",
    "\n",
    "s_sam = problem.s_dist.sample(n_s, seed=27)\n",
    "b_sam = problem.b_dist.sample(n_s, seed=27)\n",
    "with tf.Session() as sess:\n",
    "  b_sample = sess.run(b_sam, phs)\n",
    "  s_sample = sess.run(s_sam, phs)\n",
    "\n",
    "labels = [r\"$x_0$\", \"$x_1$\", \"$x_2$\"]\n",
    "ran = [(-10,10),\n",
    "       (-10,10),\n",
    "       (0,4)]\n",
    "bins = [np.linspace(-10,10,40, endpoint=True),\n",
    "        np.linspace(-10,10,40, endpoint=True),\n",
    "        np.linspace(0,4,20, endpoint=True)]\n",
    "\n",
    "# 2D levels to use\n",
    "levels = 1.0 - np.exp(-0.5 * np.array([1.,2.,3.]) ** 2)\n",
    "\n",
    "fig = corner.corner(b_sample, bins=bins,range=ran, color=\"blue\",weights=np.ones(n_s),\n",
    "                    smooth=0.95, labels=labels,levels=levels,plot_datapoints=False)\n",
    "fig = corner.corner(s_sample, bins=bins,range=ran, color=\"red\",weights=np.ones(n_s),\n",
    "                    smooth=0.95, labels=labels,levels=levels,plot_datapoints=False,\n",
    "                    fig=fig)\n",
    "\n",
    "# set 1D hist y-scales\n",
    "scales = [12000, 12000, 25000]\n",
    "n_dim = 3\n",
    "axes = np.array(fig.axes).reshape((n_dim, n_dim))\n",
    "for i in range(n_dim):\n",
    "  ax = axes[i, i]\n",
    "  ax.set_ylim([0,scales[i]])\n",
    "\n",
    "#fig.savefig(\"../paper/gfx/figure2a.pdf\",bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_s = 50000\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  weights=np.ones(s_sample.shape[0])\n",
    "  m_sample = sess.run(problem.m_dist.sample(n_s, seed=17), phs)\n",
    "\n",
    "labels = [r\"$x_0$\", \"$x_1$\", \"$x_2$\"]\n",
    "ran = [(-10,10),\n",
    "       (-10,10),\n",
    "       (0,4)]\n",
    "bins = [np.linspace(-10,10,40, endpoint=True),\n",
    "        np.linspace(-10,10,40, endpoint=True),\n",
    "        np.linspace(0,4,20, endpoint=True)]\n",
    "\n",
    "# 2D levels to use\n",
    "levels = 1.0 - np.exp(-0.5 * np.array([1.,2.,3.]) ** 2)\n",
    "\n",
    "fig = corner.corner(m_sample, bins=bins, range=ran,\n",
    "                    levels=levels,color=\"black\",weights=np.ones(n_s),\n",
    "                    smooth=0.9999, labels=labels,plot_datapoints=False)\n",
    "\n",
    "# set 1D hist y-scales\n",
    "scales = [12000, 12000, 25000]\n",
    "n_dim = 3\n",
    "axes = np.array(fig.axes).reshape((n_dim, n_dim))\n",
    "for i in range(n_dim):\n",
    "  ax = axes[i, i]\n",
    "  ax.set_ylim([0,scales[i]])\n",
    "\n",
    "#fig.savefig(\"../paper/gfx/figure2b.pdf\",bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "  train_arrays = sess.run(problem.train_data())\n",
    "  valid_arrays = sess.run(problem.valid_data())   \n",
    "  test_arrays =  sess.run(problem.test_data())   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synthetic_3D_cross_entropy import SyntheticThreeDimCrossEntropy\n",
    "\n",
    "seed = 17\n",
    "\n",
    "n_epochs = 100\n",
    "lr = 1e-2\n",
    "batch_size = 1000\n",
    "xe_path = f\"xe_lr_{lr}_n_e_{n_epochs}_seed_{seed}\"\n",
    "\n",
    "xe_model = SyntheticThreeDimCrossEntropy(model_path=xe_path,\n",
    "                                         seed=seed)\n",
    "\n",
    "xe_model.fit(n_epochs=n_epochs, lr=lr,batch_size=batch_size, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc = SummaryStatisticComputer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_shapes = ssc.classifier_shapes(xe_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from template_model import TemplateModel\n",
    "\n",
    "def compute_fisher(shapes):\n",
    "  with tf.Session() as sess:\n",
    "    tm = TemplateModel()\n",
    "    tm.templates_from_dict(shapes)\n",
    "    fisher = tm.asimov_hess()\n",
    "  return fisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_fish = compute_fisher(clf_shapes)\n",
    "clf_fish.marginals([\"s_exp\",\"r_dist\",\"b_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_variation_plot(shape_dict):\n",
    "  fig, ax = plt.subplots(2,1, figsize=(8,8),sharex=True)\n",
    "  fig.subplots_adjust(hspace = 0.05)\n",
    "  \n",
    "  \n",
    "  bins = np.linspace(0,1,11,endpoint=True)\n",
    "  centers =   (bins[1:]+bins[:-1])/2.\n",
    "  width = (bins[1:]-bins[:-1])\n",
    "  \n",
    "  plot_options = {(\"sig\",) : (r\"signal\",\"red\"),\n",
    "                 (\"bkg\",2.,3.) : (r\"background\",\"blue\")}\n",
    "                 \n",
    "  for k, option in plot_options.items():\n",
    "    clf_shapes_norm = shape_dict[k]/shape_dict[k].sum()\n",
    "    ax[0].bar(x=centers, height=clf_shapes_norm,\n",
    "              width=width,color=option[1], alpha=0.35,\n",
    "              label=option[0])\n",
    "  \n",
    "  exp_bkg =  (shape_dict[(\"bkg\",2.,3.)]/shape_dict[(\"bkg\",2.,3.)].sum())*1000.\n",
    "  exp_sig = (shape_dict[(\"sig\",)]/shape_dict[(\"sig\",)].sum())*50.\n",
    "  sig_art = ax[1].bar(x=centers, height=exp_sig/exp_bkg,width=width,\n",
    "                      color=\"red\", label=\"signal\", alpha=0.35)\n",
    "  \n",
    "  plot_options = {(\"bkg\",2.0,2.5) : (r\"v\",\"green\"),\n",
    "                  (\"bkg\",2.0,3.5) : (r\"^\",\"green\"),\n",
    "                  (\"bkg\",1.8,3.0) : (r\"v\",\"purple\"),\n",
    "                  (\"bkg\",2.2,3.0) : (r\"^\",\"purple\")}\n",
    "  \n",
    "  var_lines = []\n",
    "  var_labels = []\n",
    "  for k, option in plot_options.items():\n",
    "    exp_shift_bkg = (shape_dict[k]/shape_dict[k].sum())*1000.\n",
    "    label = f\"bkg $r={k[1]}$ $\\\\lambda={k[2]}$\"\n",
    "    var_lines.append(ax[1].errorbar(x=centers, y=(exp_shift_bkg-exp_bkg)/exp_bkg,\n",
    "                     xerr=width/2.,fmt=\".\", color=option[1],\n",
    "                     alpha=0.55, marker=option[0]))\n",
    "    var_labels.append(label)\n",
    "  \n",
    "  ax[0].legend(loc=\"upper center\", frameon=False)\n",
    "  ax[0].set_ylim(top=0.7)\n",
    "  ax[0].set_ylabel(\"counts (normalised)\")\n",
    "  \n",
    "  leg_0 = ax[1].legend(var_lines,var_labels,loc=\"lower center\", ncol=2,frameon=False)\n",
    "  leg_1 = ax[1].legend([sig_art],[\"signal\"],loc=\"upper center\", frameon=False)\n",
    "\n",
    "  ax[1].set_ylim([-0.75,0.75])\n",
    "  ax[1].set_xlim([0.,1.])\n",
    "  \n",
    "  ax[1].add_artist(leg_0)\n",
    "  ax[1].set_ylabel(\"relative variation\")\n",
    "  ax[1].yaxis.set_major_locator(matplotlib.ticker.MultipleLocator(0.2))\n",
    "  \n",
    "  fig.align_ylabels()\n",
    "\n",
    "  \n",
    "  return ax, fig\n",
    " \n",
    "axs, fig = shape_variation_plot(clf_shapes)\n",
    "axs[1].set_xlabel(\"neural network classifier output\")\n",
    "#fig.savefig(\"../paper/gfx/figure3a.pdf\",bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytical likelihood ratio\n",
    "\n",
    "Probabilistic classifiers trained to differenciate signal and background observation can be used to approximate the likelihood ratio. In this\n",
    "particular case, we can easily approximate the likelihood ratio analytically.\n",
    "\n",
    "$$s(\\textbf{x}) = \\frac{s(\\textbf{x})}{s(\\textbf{x}) + b(\\textbf{x})}$$\n",
    "\n",
    "where $s$ is fully specified but $b$ depends on $r$ and $\\lambda$, which for\n",
    "a probabilistic classifier are typically taken as fixed at certain values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_shapes = ssc.optimal_shapes()\n",
    "opt_fish = compute_fisher(opt_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_fish.marginals([\"s_exp\",\"r_dist\",\"b_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axs, fig = shape_variation_plot(opt_shapes)\n",
    "\n",
    "axs[1].set_xlabel(\"optimal classifier output\")\n",
    "#fig.savefig(\"../paper/gfx/figure3b.pdf\",bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INFERNO training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synthetic_3D_inferno import SyntheticThreeDimInferno\n",
    "\n",
    "n_epochs = 100\n",
    "lr = 1e-6\n",
    "batch_size = 1000\n",
    "t_train = 0.1\n",
    "t_eval = 0.05\n",
    "\n",
    "n_inits = 100\n",
    "seed = 7\n",
    "\n",
    "\n",
    "pars = [\"s_exp\", \"r_dist\", \"b_rate\"]\n",
    "\n",
    "aux = {\"r_dist\": ds.Normal(loc=2.0, scale=0.4),\n",
    "       \"b_rate\": ds.Normal(loc=3.0, scale=1.)}\n",
    "\n",
    "\n",
    "inf_path = f\"inf_ne_{n_epochs}_lr_{lr}_bs_{batch_size}_t_{t_train}\"\n",
    "\n",
    "inferno = SyntheticThreeDimInferno(model_path=inf_path, poi=\"s_exp\",\n",
    "                                    pars=pars, seed=seed, aux=aux)\n",
    "inferno.fit(n_epochs=n_epochs, lr=lr, batch_size=batch_size,\n",
    "            temperature=t_train, seed=seed)\n",
    "\n",
    "inf_fisher, inf_aux_fisher = inferno.eval_hessian(temperature=t_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferno.history[\"loss_valid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_fisher.marginals(pars[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_shapes = ssc.inferno_shapes(inf_path)\n",
    "inf_fish = compute_fisher(inf_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_fish.marginals([\"s_exp\",\"r_dist\",\"b_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axs, fig = shape_variation_plot(inf_shapes)\n",
    "\n",
    "axs[1].set_xlabel(\"inferno output\")\n",
    "#axs[0].set_ylim(top=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytical Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extended_model import ExtendedModel\n",
    "\n",
    "problem = SyntheticThreeDimExample()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = tf.placeholder(dtype=tf.float32, shape=(None, 3), name=\"x_values\")\n",
    "\n",
    "aux = {\"r_dist\" : ds.Normal(loc=2.0, scale=0.4),\n",
    "       \"b_rate\" : ds.Normal(loc=3.0, scale=1.0),\n",
    "       \"b_exp\" : ds.Normal(loc=1000.0, scale=100.)}\n",
    "em = ExtendedModel(problem, aux=aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_t = problem.transform_bkg(x_values)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  valid_arrays = sess.run(problem.valid_data())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "  bkg_t_arr = sess.run(bkg_t, {x_values : valid_arrays[\"bkg\"]})\n",
    "  obs_phs = {em.s_n_exp : 50.,\n",
    "               em.b_n_exp : 1000.,\n",
    "               em.s_data : valid_arrays[\"sig\"],\n",
    "               em.b_data : bkg_t_arr }\n",
    "  e_hess, e_hess_aux = em.hess(par_phs={},obs_phs=obs_phs)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_hess.marginals([\"s_exp\",\"r_dist\",\"b_rate\"])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "3D_analytical_problem.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
